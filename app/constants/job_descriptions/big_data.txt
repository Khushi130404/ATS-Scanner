We are looking for a Big Data Engineer responsible for designing, building, and maintaining scalable data pipelines. 
Responsibilities include processing large datasets, developing ETL workflows, and implementing data solutions across distributed systems. 
Required skills: Hadoop, Spark, Hive, Kafka, HBase, Python, SQL, AWS.